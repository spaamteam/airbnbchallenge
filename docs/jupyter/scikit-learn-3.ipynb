{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection: Choosing Estimators and Their Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cross-validation](./img/cross_validation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score, and Cross-validated Scores\n",
    "\n",
    "> As we have seen, every estimator exposes a score method that can judge the quality of the fit (or the prediction) on new data. Bigger is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, svm\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__k-fold Cross-validation__: [look here](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "> To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in folds that we use for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]\n"
     ]
    }
   ],
   "source": [
    "# Using k-fold cross-validation\n",
    "import numpy as np\n",
    "\n",
    "X_folds = np.array_split(X_digits, 3)\n",
    "y_folds = np.array_split(y_digits, 3)\n",
    "scores = list()\n",
    "\n",
    "for k in range(3):\n",
    "    # We use 'list' to copy, in order to 'pop' later on\n",
    "    X_train = list(X_folds)\n",
    "    X_test  = X_train.pop(k)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = list(y_folds)\n",
    "    y_test  = y_train.pop(k)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation generators\n",
    "\n",
    "> The code above to split data in train and test sets is tedious to write. Scikit-learn exposes cross-validation generators to generate list of indices for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5] | test: [0 1]\n",
      "Train: [0 1 4 5] | test: [2 3]\n",
      "Train: [0 1 2 3] | test: [4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "k_fold = cross_validation.KFold(n=6, n_folds=3)\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "     print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cross-validation can then be implemented easily\n",
    "kfold = cross_validation.KFold(len(X_digits), n_folds=3)\n",
    "\n",
    "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])\n",
    "    for train, test in kfold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93489149,  0.95659432,  0.93989983])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To compute the score method of an estimator, the sklearn exposes a helper function\n",
    "cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAADYCAYAAAAwEfyMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhi/j+jUi7Yq3YnkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e861fdb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Cross-validation with an SVM on the Digits dataset\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, datasets, svm\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "C_s = np.logspace(-10, 0, 10)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "for C in C_s:\n",
    "    svc.C = C\n",
    "    this_scores = cross_validation.cross_val_score(svc, X, y, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "# Do the plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.semilogx(C_s, scores)\n",
    "plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\n",
    "plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\n",
    "locs, labels = plt.yticks()\n",
    "plt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search and Cross-validated Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Grid Search__: [look here](https://en.wikipedia.org/wiki/Hyperparameter_optimization)\n",
    "\n",
    "> The sklearn provides an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=-1,\n",
       "       param_grid={'C': array([  1.00000e-06,   3.59381e-06,   1.29155e-05,   4.64159e-05,\n",
       "         1.66810e-04,   5.99484e-04,   2.15443e-03,   7.74264e-03,\n",
       "         2.78256e-02,   1.00000e-01])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),\n",
    "    n_jobs=-1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92500000000000004"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0077426368268112772"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94353826850690092"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction performance on test set is not as good as on train set\n",
    "clf.score(X_digits[1000:], y_digits[1000:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Nested Cross-validation__\n",
    "\n",
    "> Two cross-validation loops are performed in parallel: one by the GridSearchCV estimator to set gamma and the other one by cross_val_score to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93853821,  0.96327212,  0.94463087])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(clf, X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cross-validated Estimators__\n",
    "\n",
    "> Cross-validation to set a parameter can be done more efficiently on an algorithm-by-algorithm basis. This is why for certain estimators the sklearn exposes Cross-validation: evaluating estimator performance estimators that set their parameter automatically by cross-validation.\n",
    "\n",
    "> These estimators are called similarly to their counterparts, with ‘CV’ appended to their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
       "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "lasso = linear_model.LassoCV()\n",
    "lasso.fit(X_diabetes, y_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Seeking Representations of the Data\n",
    "\n",
    "![Clustering](./img/clustering.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering: grouping observations together\n",
    "\n",
    "For more, see [here](https://en.wikipedia.org/wiki/Cluster_analysis).\n",
    "\n",
    "__Problem Solving with Clustering__\n",
    "\n",
    "> Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a clustering task: split the observations into well-separated group called clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__k-Means Clustering__: [look here](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "\n",
    "> Note that there exist a lot of different clustering criteria and associated algorithms. The simplest clustering algorithm is K-means.\n",
    "\n",
    "> There is absolutely no guarantee of recovering a ground truth... don’t over-interpret clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cluster, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Clusters\n",
    "print(k_means.labels_[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Actual classes\n",
    "print(y_iris[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Vector Quantization__: [look here](https://en.wikipedia.org/wiki/Vector_quantization)\n",
    "\n",
    "> Clustering in general and KMeans, in particular, can be seen as a way of choosing a small number of exemplars to compress the information. The problem is sometimes known as vector quantization. For instance, this can be used to posterize an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=5, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "   lena = sp.lena()\n",
    "except AttributeError:\n",
    "   from scipy import misc\n",
    "   lena = misc.lena()\n",
    "\n",
    "X = lena.reshape((-1, 1)) # We need an (n_sample, n_feature) array\n",
    "k_means = cluster.KMeans(n_clusters=5, n_init=1)\n",
    "k_means.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = k_means.cluster_centers_.squeeze()\n",
    "labels = k_means.labels_\n",
    "lena_compressed = np.choose(labels, values)\n",
    "lena_compressed.shape = lena.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hierarchical Clustering: Ward__\n",
    "\n",
    "* _Agglomerative_: Bottom up\n",
    "* _Divisive_: Top down\n",
    "\n",
    "__Connectivity-constrained Clustering__\n",
    "\n",
    "> With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in the scikit are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute structured hierarchical clustering...\n",
      "('Elapsed time: ', 10.0781090259552)\n",
      "('Number of pixels: ', 65536)\n",
      "('Number of clusters: ', 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import time\n",
    "\n",
    "# Generate data\n",
    "lena = sp.misc.lena()\n",
    "\n",
    "# Downsample the image by a factor of 4\n",
    "lena = lena[::2, ::2] + lena[1::2, ::2] + lena[::2, 1::2] + lena[1::2, 1::2]\n",
    "X = np.reshape(lena, (-1, 1))\n",
    "\n",
    "# Define the structure A of the data. Pixels connected to their neighbors.\n",
    "connectivity = grid_to_graph(*lena.shape)\n",
    "\n",
    "# Compute clustering\n",
    "print(\"Compute structured hierarchical clustering...\")\n",
    "st = time.time()\n",
    "n_clusters = 15  # number of regions\n",
    "ward = AgglomerativeClustering(n_clusters=n_clusters, \n",
    "    linkage='ward', connectivity=connectivity).fit(X)\n",
    "label = np.reshape(ward.labels_, lena.shape)\n",
    "\n",
    "# Output results.\n",
    "print(\"Elapsed time: \", time.time() - st)\n",
    "print(\"Number of pixels: \", label.size)\n",
    "print(\"Number of clusters: \", np.unique(label).size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Agglomeration__\n",
    "\n",
    "> We have seen that sparsity could be used to mitigate the curse of dimensionality, i.e an insufficient amount of observations compared to the number of features. Another approach is to merge together similar features: feature agglomeration. This approach can be implemented by clustering in the feature direction, in other words clustering the transposed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',\n",
       "           connectivity=<64x64 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 288 stored elements in COOrdinate format>,\n",
       "           linkage='ward', memory=Memory(cachedir=None), n_clusters=32,\n",
       "           n_components=None,\n",
       "           pooling_func=<function mean at 0x7f076812aa28>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "images = digits.images\n",
    "X = np.reshape(images, (len(images), -1))\n",
    "connectivity = grid_to_graph(*images[0].shape)\n",
    "\n",
    "agglo = cluster.FeatureAgglomeration(connectivity=connectivity, n_clusters=32)\n",
    "agglo.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_reduced = agglo.transform(X)\n",
    "X_approx = agglo.inverse_transform(X_reduced)\n",
    "images_approx = np.reshape(X_approx, images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Principal Component Analysis (PCA)__: [look here](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "\n",
    "> Principal component analysis (PCA) selects the successive components that explain the maximum variance in the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=None, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When used to transform data, PCA can reduce the dimensionality of the data by \n",
    "# projecting on a principal subspace.\n",
    "\n",
    "# Create a signal with only 2 useful dimensions\n",
    "x1 = np.random.normal(size=100)\n",
    "x2 = np.random.normal(size=100)\n",
    "x3 = x1 + x2\n",
    "X = np.c_[x1, x2, x3]\n",
    "\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.09943175e+00   6.21013970e-01   2.47599660e-32]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see, only the 2 first components are useful\n",
    "pca.n_components = 2\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Independent Component Analysis (ICA)__: [look here](https://en.wikipedia.org/wiki/Independent_component_analysis)\n",
    "\n",
    "> Independent component analysis (ICA) selects components so that the distribution of their loadings carries a maximum amount of independent information. It is able to recover non-Gaussian independent signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sample data\n",
    "time = np.linspace(0, 10, 2000)\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
    "S = np.c_[s1, s2]\n",
    "S += 0.2 * np.random.normal(size=S.shape)  # Add noise\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "\n",
    "# Mix data\n",
    "A = np.array([[1, 1], [0.5, 2]])  # Mixing matrix\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    "# Compute ICA\n",
    "ica = decomposition.FastICA()\n",
    "S_ = ica.fit_transform(X)  # Get the estimated sources\n",
    "A_ = ica.mixing_.T\n",
    "np.allclose(X,  np.dot(S_, A_) + ica.mean_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
